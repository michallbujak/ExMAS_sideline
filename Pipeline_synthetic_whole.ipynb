{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bb66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "\n",
    "import ExMAS\n",
    "import utils\n",
    "from ExMAS.utils import inData as inData\n",
    "from torch.utils.data import  DataLoader\n",
    "import seaborn as sns\n",
    "import ExMAS_wrapper as ExMASFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf24d03",
   "metadata": {},
   "source": [
    "Load run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a39932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_parameters = utils.get_initial_parameters('run_parameters_v2.json')\n",
    "initial_params = ExMAS.utils.get_config(run_parameters.initial_parameters)\n",
    "initial_params.duration_in_minutes = run_parameters.duration_in_minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc4a413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand size:  10\n"
     ]
    }
   ],
   "source": [
    "demand_structure_requests = ExMASFunc.split_demand_structure(run_parameters)\n",
    "print(\"Demand size: \", len(run_parameters.values_of_demand_variables[0]) * run_parameters.replications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed41870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b728fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_dotmaps = [pool.apply(ExMASFunc.demand_generation, args=(initial_params, demand, True))\n",
    "                  for demand in demand_structure_requests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13fd549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>treq</th>\n",
       "      <th>tdep</th>\n",
       "      <th>ttrav</th>\n",
       "      <th>tarr</th>\n",
       "      <th>tdrop</th>\n",
       "      <th>dist</th>\n",
       "      <th>pax_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5715066940</td>\n",
       "      <td>4594247438</td>\n",
       "      <td>2022-02-28 15:00:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:52:23</td>\n",
       "      <td>2022-02-28 15:52:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3143</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>44854736</td>\n",
       "      <td>894421661</td>\n",
       "      <td>2022-02-28 15:00:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:31:36</td>\n",
       "      <td>2022-02-28 15:32:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1896</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1583992769</td>\n",
       "      <td>44792321</td>\n",
       "      <td>2022-02-28 15:00:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:54:03</td>\n",
       "      <td>2022-02-28 15:54:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3243</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2598004622</td>\n",
       "      <td>1436427141</td>\n",
       "      <td>2022-02-28 15:01:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:17:36</td>\n",
       "      <td>2022-02-28 15:18:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1056</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>44813599</td>\n",
       "      <td>44838797</td>\n",
       "      <td>2022-02-28 15:01:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:55:29</td>\n",
       "      <td>2022-02-28 15:57:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3329</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        origin  destination                treq tdep           ttrav  \\\n",
       "60  5715066940   4594247438 2022-02-28 15:00:17  NaN 0 days 00:52:23   \n",
       "38    44854736    894421661 2022-02-28 15:00:39  NaN 0 days 00:31:36   \n",
       "23  1583992769     44792321 2022-02-28 15:00:52  NaN 0 days 00:54:03   \n",
       "34  2598004622   1436427141 2022-02-28 15:01:15  NaN 0 days 00:17:36   \n",
       "37    44813599     44838797 2022-02-28 15:01:46  NaN 0 days 00:55:29   \n",
       "\n",
       "                  tarr tdrop  dist  pax_id  \n",
       "60 2022-02-28 15:52:40   NaN  3143      60  \n",
       "38 2022-02-28 15:32:15   NaN  1896      38  \n",
       "23 2022-02-28 15:54:55   NaN  3243      23  \n",
       "34 2022-02-28 15:18:51   NaN  1056      34  \n",
       "37 2022-02-28 15:57:15   NaN  3329      37  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand_dotmaps[0].requests[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcabe25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmas_res = [pool.apply(ExMAS.main, args=(data, initial_params, False)) for data in demand_dotmaps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd334f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e9336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmas_res_extracted = [ExMASFunc.alter_kpis(data.sblts.res)['SavedVehHours'] for data in exmas_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0237da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27198158008346524,\n",
       " 0.23051881993896237,\n",
       " 0.2744986757472569,\n",
       " 0.25986971969996053,\n",
       " 0.21050991795617405]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exmas_res_extracted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ee495d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>treq</th>\n",
       "      <th>tdep</th>\n",
       "      <th>ttrav</th>\n",
       "      <th>tarr</th>\n",
       "      <th>tdrop</th>\n",
       "      <th>dist</th>\n",
       "      <th>pax_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5715066940</td>\n",
       "      <td>4594247438</td>\n",
       "      <td>2022-02-28 15:00:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:52:23</td>\n",
       "      <td>2022-02-28 15:52:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3143</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>44854736</td>\n",
       "      <td>894421661</td>\n",
       "      <td>2022-02-28 15:00:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:31:36</td>\n",
       "      <td>2022-02-28 15:32:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1896</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1583992769</td>\n",
       "      <td>44792321</td>\n",
       "      <td>2022-02-28 15:00:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:54:03</td>\n",
       "      <td>2022-02-28 15:54:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3243</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        origin  destination                treq tdep           ttrav  \\\n",
       "60  5715066940   4594247438 2022-02-28 15:00:17  NaN 0 days 00:52:23   \n",
       "38    44854736    894421661 2022-02-28 15:00:39  NaN 0 days 00:31:36   \n",
       "23  1583992769     44792321 2022-02-28 15:00:52  NaN 0 days 00:54:03   \n",
       "\n",
       "                  tarr tdrop  dist  pax_id  \n",
       "60 2022-02-28 15:52:40   NaN  3143      60  \n",
       "38 2022-02-28 15:32:15   NaN  1896      38  \n",
       "23 2022-02-28 15:54:55   NaN  3243      23  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand = [dem.requests for dem in demand_dotmaps]\n",
    "demand[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3a28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('demand.obj', 'wb') as demand_file:\n",
    "  pickle.dump(demand, demand_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4348e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kpis.obj', 'wb') as kpis_file:\n",
    "  pickle.dump(exmas_res_extracted, kpis_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6f7a8f",
   "metadata": {
    "tags": [
     "\"hide-output\"",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ExMAS.utils.load_G(inData, initial_params, stats=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e75ea9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "origins, destinations = zip(*[utils.split_geo_positions(item, inData) for item in demand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14db8a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>time_request</th>\n",
       "      <th>x_geo</th>\n",
       "      <th>y_geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5715066940</td>\n",
       "      <td>2022-02-28 15:00:17</td>\n",
       "      <td>4.359206</td>\n",
       "      <td>51.987170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44854736</td>\n",
       "      <td>2022-02-28 15:00:39</td>\n",
       "      <td>4.347923</td>\n",
       "      <td>52.012532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1583992769</td>\n",
       "      <td>2022-02-28 15:00:52</td>\n",
       "      <td>4.351989</td>\n",
       "      <td>51.984965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       origin        time_request     x_geo      y_geo\n",
       "0  5715066940 2022-02-28 15:00:17  4.359206  51.987170\n",
       "1    44854736 2022-02-28 15:00:39  4.347923  52.012532\n",
       "2  1583992769 2022-02-28 15:00:52  4.351989  51.984965"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origins[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b3a7c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>time_request</th>\n",
       "      <th>x_geo</th>\n",
       "      <th>y_geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4594247438</td>\n",
       "      <td>2022-02-28 15:00:17</td>\n",
       "      <td>4.355876</td>\n",
       "      <td>52.009397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>894421661</td>\n",
       "      <td>2022-02-28 15:00:39</td>\n",
       "      <td>4.343945</td>\n",
       "      <td>52.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44792321</td>\n",
       "      <td>2022-02-28 15:00:52</td>\n",
       "      <td>4.357743</td>\n",
       "      <td>51.998695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   destination        time_request     x_geo      y_geo\n",
       "0   4594247438 2022-02-28 15:00:17  4.355876  52.009397\n",
       "1    894421661 2022-02-28 15:00:39  4.343945  52.000152\n",
       "2     44792321 2022-02-28 15:00:52  4.357743  51.998695"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983e2d4",
   "metadata": {},
   "source": [
    "Geo-pandas - poprawa generacji grida; zapytania przestrzenne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a0d11",
   "metadata": {},
   "source": [
    "### Choose one of the approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5353888",
   "metadata": {},
   "source": [
    "#### Multichannel approach - corresponding to time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "036d47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = utils.create_time_space_grid(inData, number_of_space_points=10, parameters=initial_params)\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "map_origins = [pool.apply(utils.apply_time_space_grid, args=(data, grid)) for data in origins]\n",
    "map_destinations = [pool.apply(utils.apply_time_space_grid, args=(data, grid)) for data in destinations]\n",
    "pool.close()\n",
    "\n",
    "demand_list_tensors = utils.merge_origins_destinations(map_origins, map_destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c547b",
   "metadata": {},
   "source": [
    "#### Snapshot approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19bc2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = utils.create_space_grid(inData, 10)\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "map_origins = [pool.apply(utils.apply_space_grid, args=(data, grid)) for data in origins]\n",
    "map_destinations = [pool.apply(utils.apply_space_grid, args=(data, grid)) for data in destinations]\n",
    "pool.close()\n",
    "\n",
    "demand_list_tensors = utils.merge_origins_destinations(map_origins, map_destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958e983",
   "metadata": {},
   "source": [
    "### End of difference between approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0207ffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand_list_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "785bae4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  2.,  1.,  4.,  5.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  2.,  0.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  2.,  5.,  2.,  0.,  1.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  2.,  1.,  3.,  2.,  1.,  1.,  2.,  0.,  0.],\n",
       "         [ 2.,  1.,  1.,  4.,  1.,  0.,  3.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  3.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  2.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  1.,  3.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  2.,  3., 12.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  3.,  2.,  9.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  3.,  0.,  1.,  1.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand_list_tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b9e776a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2720)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list_tensors = [torch.tensor(a) for a in exmas_res_extracted]\n",
    "results_list_tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94231441",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = utils.WholeDataset(demand_list_tensors, results_list_tensors)\n",
    "train_size = int(0.8 * len(whole_dataset))\n",
    "test_size = len(whole_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(whole_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca75b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_dataset.pt')\n",
    "torch.save(test_dataset, 'test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbb7489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load('train_dataset.pt')\n",
    "test_dataset = torch.load('test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02bf7274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kpis = [train_dataset.dataset.labels[t] for t in train_dataset.indices]\n",
    "plt.hist(kpis, bins=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d30c8",
   "metadata": {},
   "source": [
    "More sofisticated VAE can be taken from: https://github.com/AntixK/PyTorch-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac73a58",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d89bd",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1e73bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "7294f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "x_dim  = 2*10*10\n",
    "hidden_dim = 100\n",
    "latent_dim = 32\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 30\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e97ef7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "60fb4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b7cd4ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 10, 10])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(train_loader))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b70eb2",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "cfb8ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_       = self.LeakyReLU(self.FC_input(x))\n",
    "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_)                     \n",
    "        \n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "dcbdf371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_channels):\n",
    "        super(Encoder2, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        hidden_dims = [32, 64, 128, 256, 512,1000]\n",
    "        in_channels = input_channels\n",
    "\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "#         self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "#         self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.encoder(x)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcf73a",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0a043b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h     = self.LeakyReLU(self.FC_hidden(x))\n",
    "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        \n",
    "        x_hat = torch.sigmoid(self.FC_output(h))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ee881bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder2(nn.Module):\n",
    "    def __init__(self, latent_dim, batch_size):\n",
    "        super(Decoder2, self).__init__()\n",
    "        \n",
    "        hidden_dims = [32, 64, 128, 256, 512]\n",
    "        hidden_dims.reverse()\n",
    "        \n",
    "#         self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*10*10)\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1])\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 2,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('Input for decoder: ', x.shape)\n",
    "        result = self.decoder_input(x)\n",
    "        print('Decoder_input:', result.shape)\n",
    "#         result = result.view(-1, 512, 2, 2)\n",
    "#         result = result.view(-1, 2, 10, 10)\n",
    "        print('Results shape:', result.shape)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fa5a2",
   "metadata": {},
   "source": [
    "### Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f6e586b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z\n",
    "                  \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        x_hat = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbe1ef",
   "metadata": {},
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2ed124f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "encoder2 = Encoder2(latent_dim=20, input_channels=2)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
    "decoder2 = Decoder2(latent_dim=20, batch_size=batch_size)\n",
    "\n",
    "autoencoder = VAE(Encoder=encoder2, Decoder=decoder2).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f713d2",
   "metadata": {},
   "source": [
    "## Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "cd5b204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss + KLD\n",
    "\n",
    "\n",
    "optimizer = Adam(autoencoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055baf2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "435bbc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VAE...\n",
      "Input for decoder:  torch.Size([4, 20])\n",
      "Decoder_input: torch.Size([4, 32])\n",
      "Results shape: torch.Size([4, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [512, 256, 3, 3], but got 2-dimensional input of size [4, 32] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7496/2871788618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_hat: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ox\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7496/926656139.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# takes exponential function (log var -> var)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ox\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7496/1663754623.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m#         result = result.view(-1, 2, 10, 10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Results shape:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ox\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ox\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ox\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ox\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ox\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ox\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    921\u001b[0m             input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         return F.conv_transpose2d(\n\u001b[0m\u001b[0;32m    924\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             output_padding, self.groups, self.dilation)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [512, 256, 3, 3], but got 2-dimensional input of size [4, 32] instead"
     ]
    }
   ],
   "source": [
    "print(\"Start training VAE...\")\n",
    "autoencoder.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "#         x = x.view(batch_size, x_dim)\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = autoencoder(x)\n",
    "        print('x: ', x.shape, '/n')\n",
    "        print('x_hat: ', x_hat.shape, '/n')\n",
    "        loss = loss_function(x, x_hat, mean, log_var)\n",
    "        \n",
    "        overall_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size))\n",
    "    \n",
    "print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce88bca",
   "metadata": {},
   "source": [
    "## New representation, example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f18cd1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 3., 2., 2., 2., 2., 0., 0., 0., 0., 0., 1., 4., 2., 3., 1.,\n",
      "         0., 0., 0., 0., 0., 2., 3., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         2., 1., 0., 0., 0., 0., 0., 2., 1., 6., 1., 1., 1., 0., 0., 0., 1., 2.,\n",
      "         2., 0., 3., 2., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 3., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 2., 1.,\n",
      "         3., 0., 0., 0., 0., 0., 0., 3., 5., 9., 2., 0., 0., 0., 0., 0., 2., 0.,\n",
      "         3., 7., 5., 0., 0., 0., 0., 0., 0., 2., 2., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 2., 4., 0., 5., 1., 0., 0., 0., 0., 0., 1., 3., 0., 3., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 3., 3., 0., 0., 0., 0., 0., 0., 0., 2., 3., 2.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 5., 1., 3., 1., 0., 0., 0., 2., 3.,\n",
      "         2., 2., 1., 0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 2., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 2., 3.,\n",
      "         5., 0., 0., 0., 0., 0., 0., 1., 2., 8., 2., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         4., 7., 0., 1., 1., 0., 0., 0., 0., 2., 1., 5., 1., 1., 2., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 3., 1., 1., 3., 1., 0., 0., 0., 0., 0., 1., 2., 2., 2., 3.,\n",
      "         0., 0., 0., 0., 0., 0., 5., 3., 1., 0., 0., 0., 0., 0., 0., 2., 2., 1.,\n",
      "         5., 0., 1., 0., 0., 0., 0., 1., 2., 4., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "         3., 2., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 2., 4., 6.,\n",
      "         2., 0., 0., 0., 0., 0., 0., 2., 2., 7., 2., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         5., 5., 2., 3., 0., 0., 0., 0., 0., 2., 6., 3., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 1., 3., 3., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 3., 0., 4., 0., 0., 1., 0., 0., 0., 2., 3., 5.,\n",
      "         3., 0., 0., 1., 0., 0., 2., 0., 1., 5., 0., 1., 2., 0., 0., 0., 1., 1.,\n",
      "         1., 2., 0., 2., 2., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 4., 1., 0., 2., 0., 0., 0., 0., 0., 2., 3., 4.,\n",
      "         4., 0., 0., 0., 0., 0., 0., 0., 2., 5., 1., 0., 0., 0., 0., 0., 0., 3.,\n",
      "         3., 4., 2., 3., 1., 0., 0., 0., 1., 2., 2., 4., 2., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0') \n",
      "\n",
      "Modelled hidden: \n",
      " mean:  tensor([[ 1.6064, -0.8065,  1.3395,  1.0935, -0.1216, -1.0722, -2.5837,  1.6342,\n",
      "         -0.1762, -2.3866, -2.9605, -0.4884, -0.1799,  1.0088, -1.2494, -0.7845,\n",
      "         -0.1931,  1.6072,  1.8118,  3.4291],\n",
      "        [ 1.4843, -0.7589,  1.3041,  1.1353, -0.0830, -0.9309, -2.5516,  1.7713,\n",
      "         -0.3630, -2.2610, -3.0482, -0.5567, -0.1845,  1.0065, -1.1368, -0.7547,\n",
      "         -0.0884,  1.6347,  1.9154,  3.2603],\n",
      "        [ 1.7089, -0.9468,  1.4160,  1.1277, -0.1437, -1.0064, -2.7244,  1.7036,\n",
      "         -0.2999, -2.3057, -3.2813, -0.6160, -0.1287,  1.1930, -1.3316, -0.7710,\n",
      "          0.0180,  1.5854,  2.0223,  3.4854],\n",
      "        [ 1.7235, -0.8525,  1.2846,  1.1469, -0.1436, -0.9433, -2.7172,  1.7910,\n",
      "         -0.2947, -2.3189, -3.1886, -0.6661, -0.1146,  1.0026, -1.2942, -0.8194,\n",
      "         -0.1476,  1.7609,  2.1244,  3.3935]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " log variance:  tensor([[-0.7469, -0.1531, -0.1973, -0.2994, -0.0084, -0.6034, -0.6523,  0.0466,\n",
      "          0.0415, -0.9871, -0.8510, -0.2547, -0.8799, -0.7079,  0.0862, -0.2007,\n",
      "         -0.4343, -0.7406, -0.5874, -0.4001],\n",
      "        [-0.8227, -0.1093, -0.2710, -0.2178,  0.1486, -0.5407, -0.6876,  0.0280,\n",
      "         -0.0739, -1.0305, -0.9448, -0.2220, -0.7883, -0.7391,  0.0806, -0.1882,\n",
      "         -0.4893, -0.7704, -0.5173, -0.5260],\n",
      "        [-0.7902, -0.0394, -0.1796, -0.1193,  0.0976, -0.5568, -0.6297, -0.0499,\n",
      "         -0.0387, -1.0170, -0.9980, -0.2220, -0.7890, -0.6613,  0.1019, -0.1810,\n",
      "         -0.4209, -0.7690, -0.4532, -0.5382],\n",
      "        [-0.9534, -0.0739, -0.1574, -0.1187,  0.1423, -0.6504, -0.7315, -0.1441,\n",
      "         -0.0315, -1.0072, -0.9763, -0.2601, -0.8341, -0.6977,  0.1308, -0.1444,\n",
      "         -0.4519, -0.9234, -0.4460, -0.5875]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " decoded to original: tensor([[1.1713e-06, 5.7309e-07, 1.6531e-05, 4.2244e-06, 1.6603e-06, 2.6066e-06,\n",
      "         1.0359e-06, 2.3159e-06, 3.6017e-06, 1.1595e-04, 5.4148e-08, 3.9833e-01,\n",
      "         4.7185e-01, 1.3384e-01, 1.0741e-01, 2.3893e-05, 4.0182e-06, 6.3634e-05,\n",
      "         8.8705e-08, 2.7133e-05, 3.7824e-07, 1.0000e+00, 1.0000e+00, 9.5252e-01,\n",
      "         1.0000e+00, 1.0000e+00, 6.8923e-06, 1.7279e-06, 9.2653e-07, 6.0081e-09,\n",
      "         3.1135e-06, 7.9474e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01,\n",
      "         2.3091e-01, 6.9435e-06, 1.0061e-06, 6.9451e-06, 1.9753e-06, 7.8880e-01,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 2.5112e-01, 6.3483e-06, 3.8967e-01,\n",
      "         4.7659e-05, 4.8745e-06, 1.1031e-04, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 2.8587e-01, 1.1279e-01, 1.7137e-01, 6.1927e-06, 5.1763e-05,\n",
      "         8.3138e-01, 8.2316e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         8.2000e-01, 5.0791e-01, 1.6966e-06, 2.1698e-05, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 9.9997e-01, 1.0000e+00, 9.9644e-01, 9.3886e-06,\n",
      "         1.1448e-06, 1.1283e-06, 2.9476e-06, 1.8543e-06, 1.5793e-01, 1.0000e+00,\n",
      "         6.1816e-01, 1.1411e-01, 4.6246e-07, 3.5469e-06, 7.9034e-06, 4.9990e-09,\n",
      "         1.8348e-05, 1.0039e-05, 2.8380e-06, 5.6722e-01, 7.3197e-01, 1.7833e-01,\n",
      "         8.1066e-07, 2.1313e-05, 4.1216e-05, 1.2441e-07, 2.7282e-05, 1.3155e-06,\n",
      "         3.3561e-07, 3.5628e-06, 7.4375e-07, 3.5704e-07, 1.1773e-06, 2.9115e-06,\n",
      "         4.9292e-05, 7.8750e-06, 1.8108e-07, 3.8499e-01, 9.2137e-02, 1.0943e-07,\n",
      "         1.7083e-05, 3.4462e-06, 1.3068e-05, 2.9990e-06, 2.0416e-04, 5.0404e-06,\n",
      "         2.4182e-05, 3.3941e-01, 8.1215e-01, 2.0177e-01, 9.8714e-01, 7.8201e-01,\n",
      "         5.7047e-06, 6.9320e-06, 1.5558e-07, 2.9784e-05, 1.6742e-07, 1.0000e+00,\n",
      "         1.0000e+00, 7.6104e-01, 1.0000e+00, 3.7691e-01, 1.5702e-06, 1.3735e-05,\n",
      "         1.6619e-05, 1.2867e-04, 3.2183e-06, 9.9960e-01, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.3997e-01, 3.8656e-05, 1.1892e-01, 1.0892e-05, 7.2498e-07,\n",
      "         3.9245e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.4615e-06,\n",
      "         3.9735e-06, 4.2102e-01, 1.5266e-07, 1.9849e-04, 3.0638e-01, 7.6850e-01,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.5522e-01, 1.9815e-06,\n",
      "         9.9931e-06, 2.1644e-05, 4.5394e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         7.9533e-01, 8.0808e-01, 1.8651e-01, 2.5862e-06, 1.6322e-06, 1.8347e-06,\n",
      "         7.9079e-05, 7.9827e-08, 2.9161e-06, 1.0000e+00, 3.9282e-01, 2.7488e-01,\n",
      "         2.5021e-06, 6.2351e-08, 1.2304e-05, 1.1078e-05, 1.1736e-04, 5.3526e-05,\n",
      "         3.8269e-06, 2.1952e-05, 4.3017e-05, 8.8748e-07, 7.9945e-06, 7.5974e-06,\n",
      "         4.4322e-09, 1.1842e-05],\n",
      "        [6.0292e-07, 2.7757e-07, 7.4953e-06, 2.6460e-06, 8.2449e-07, 1.2953e-06,\n",
      "         4.7636e-07, 1.1330e-06, 1.8795e-06, 7.9907e-05, 1.9605e-08, 4.2218e-01,\n",
      "         4.5585e-01, 1.3875e-01, 1.0034e-01, 1.1516e-05, 2.0593e-06, 4.5635e-05,\n",
      "         3.9675e-08, 1.7999e-05, 1.3954e-07, 1.0000e+00, 1.0000e+00, 9.5546e-01,\n",
      "         1.0000e+00, 1.0000e+00, 3.6896e-06, 6.4345e-07, 4.4413e-07, 2.1186e-09,\n",
      "         1.5649e-06, 8.2339e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         2.0175e-01, 3.8566e-06, 4.4687e-07, 3.1227e-06, 9.5618e-07, 7.8907e-01,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 2.4282e-01, 3.0725e-06, 3.9626e-01,\n",
      "         2.9920e-05, 2.0360e-06, 5.7379e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 2.8741e-01, 1.1652e-01, 1.2653e-01, 3.4709e-06, 4.0192e-05,\n",
      "         8.4267e-01, 8.4504e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         8.2429e-01, 5.2610e-01, 8.3873e-07, 1.1193e-05, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 9.9999e-01, 1.0000e+00, 9.9788e-01, 5.9811e-06,\n",
      "         5.2661e-07, 5.1008e-07, 1.8030e-06, 9.0986e-07, 1.4584e-01, 1.0000e+00,\n",
      "         6.2345e-01, 9.7140e-02, 2.3548e-07, 2.0546e-06, 4.5833e-06, 1.4288e-09,\n",
      "         8.5536e-06, 4.7255e-06, 1.5957e-06, 5.6909e-01, 7.8231e-01, 1.4946e-01,\n",
      "         4.3724e-07, 8.0294e-06, 1.7061e-05, 5.8271e-08, 1.5710e-05, 5.3990e-07,\n",
      "         1.7037e-07, 1.4296e-06, 4.6192e-07, 1.3160e-07, 6.3509e-07, 1.5781e-06,\n",
      "         2.7143e-05, 3.8728e-06, 7.7048e-08, 3.9291e-01, 7.8139e-02, 4.3012e-08,\n",
      "         8.2105e-06, 1.9271e-06, 8.5981e-06, 1.3711e-06, 1.1260e-04, 2.6812e-06,\n",
      "         1.3233e-05, 3.3921e-01, 8.0249e-01, 1.7233e-01, 9.9055e-01, 8.2064e-01,\n",
      "         2.6718e-06, 4.3646e-06, 7.5657e-08, 2.2400e-05, 8.1000e-08, 1.0000e+00,\n",
      "         1.0000e+00, 7.5524e-01, 1.0000e+00, 3.5132e-01, 7.3628e-07, 7.7037e-06,\n",
      "         8.6588e-06, 7.5860e-05, 1.3787e-06, 9.9973e-01, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.2231e-01, 2.0150e-05, 1.0739e-01, 6.7674e-06, 2.8963e-07,\n",
      "         2.0210e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.4795e-06,\n",
      "         2.0968e-06, 4.7904e-01, 6.1951e-08, 1.1311e-04, 3.7611e-01, 7.8312e-01,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.4419e-01, 1.0914e-06,\n",
      "         5.9480e-06, 1.1679e-05, 4.6276e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         8.0718e-01, 8.1979e-01, 1.6993e-01, 1.2351e-06, 7.3042e-07, 9.0364e-07,\n",
      "         4.6657e-05, 2.6886e-08, 1.3809e-06, 1.0000e+00, 4.3724e-01, 2.8558e-01,\n",
      "         1.2022e-06, 2.9336e-08, 5.3644e-06, 6.3064e-06, 7.9917e-05, 3.4728e-05,\n",
      "         1.8095e-06, 1.0864e-05, 2.5901e-05, 5.2334e-07, 5.2449e-06, 4.4151e-06,\n",
      "         1.4680e-09, 6.5661e-06],\n",
      "        [7.2511e-08, 3.0615e-08, 2.0765e-06, 3.4823e-07, 1.7030e-07, 1.9264e-07,\n",
      "         6.5848e-08, 1.5763e-07, 3.1109e-07, 1.8255e-05, 2.3386e-09, 4.1094e-01,\n",
      "         4.7963e-01, 7.2656e-02, 8.2296e-02, 1.4880e-06, 4.1856e-07, 8.6615e-06,\n",
      "         2.2273e-09, 3.4697e-06, 1.6380e-08, 1.0000e+00, 1.0000e+00, 9.7826e-01,\n",
      "         1.0000e+00, 1.0000e+00, 4.7917e-07, 1.1077e-07, 5.2212e-08, 1.7088e-10,\n",
      "         2.1092e-07, 8.2786e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         1.6754e-01, 7.6791e-07, 4.7030e-08, 6.7097e-07, 1.0872e-07, 8.7097e-01,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.9580e-01, 5.6962e-07, 3.5956e-01,\n",
      "         9.5272e-06, 3.8322e-07, 1.3296e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 2.3760e-01, 1.0576e-01, 1.4001e-01, 5.4445e-07, 7.7870e-06,\n",
      "         8.8807e-01, 8.1522e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         8.4063e-01, 5.5380e-01, 1.3562e-07, 2.5208e-06, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9893e-01, 6.9502e-07,\n",
      "         6.2455e-08, 8.3411e-08, 1.7288e-07, 1.3017e-07, 1.1105e-01, 1.0000e+00,\n",
      "         6.3368e-01, 7.0028e-02, 3.2173e-08, 2.9674e-07, 7.3759e-07, 1.0700e-10,\n",
      "         2.0489e-06, 9.5757e-07, 1.8712e-07, 6.3425e-01, 8.3686e-01, 1.1833e-01,\n",
      "         5.9780e-08, 2.3764e-06, 5.2593e-06, 4.2161e-09, 3.2601e-06, 7.8279e-08,\n",
      "         1.9289e-08, 2.4531e-07, 5.2861e-08, 1.6006e-08, 8.4696e-08, 2.3843e-07,\n",
      "         5.8765e-06, 9.2775e-07, 6.1637e-09, 3.2238e-01, 5.6652e-02, 5.8014e-09,\n",
      "         1.3011e-06, 2.4681e-07, 1.3147e-06, 3.1869e-07, 3.5990e-05, 3.7800e-07,\n",
      "         3.3373e-06, 4.0040e-01, 8.4242e-01, 1.5121e-01, 9.9539e-01, 7.9648e-01,\n",
      "         4.2440e-07, 8.0178e-07, 6.9759e-09, 6.4463e-06, 6.7991e-09, 1.0000e+00,\n",
      "         1.0000e+00, 7.7294e-01, 1.0000e+00, 3.1618e-01, 1.1787e-07, 1.6620e-06,\n",
      "         1.7356e-06, 2.3752e-05, 2.7102e-07, 9.9989e-01, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.1048e-01, 5.8376e-06, 6.5669e-02, 1.4111e-06, 4.4096e-08,\n",
      "         6.0665e-06, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.7009e-07,\n",
      "         2.6768e-07, 4.0667e-01, 5.6503e-09, 4.0427e-05, 3.4119e-01, 8.1904e-01,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.9926e-01, 1.6917e-07,\n",
      "         1.1067e-06, 2.8834e-06, 5.0021e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         8.8910e-01, 8.2137e-01, 1.1831e-01, 1.8379e-07, 7.2739e-08, 1.3305e-07,\n",
      "         1.2468e-05, 2.8028e-09, 2.0458e-07, 1.0000e+00, 3.6869e-01, 2.0791e-01,\n",
      "         2.5828e-07, 2.6421e-09, 1.7729e-06, 1.2475e-06, 3.3607e-05, 6.1568e-06,\n",
      "         3.4737e-07, 2.4351e-06, 5.7842e-06, 6.8727e-08, 5.5708e-07, 8.3550e-07,\n",
      "         8.3864e-11, 1.0045e-06],\n",
      "        [6.9446e-07, 2.7910e-07, 9.0110e-06, 2.7941e-06, 1.0861e-06, 1.2284e-06,\n",
      "         5.2389e-07, 1.4285e-06, 2.0067e-06, 8.4861e-05, 2.7658e-08, 3.8954e-01,\n",
      "         4.6500e-01, 1.2513e-01, 1.0348e-01, 1.2540e-05, 2.3857e-06, 4.1893e-05,\n",
      "         4.6686e-08, 1.9787e-05, 1.3432e-07, 1.0000e+00, 1.0000e+00, 9.5613e-01,\n",
      "         1.0000e+00, 1.0000e+00, 3.6938e-06, 7.1617e-07, 4.9258e-07, 3.1494e-09,\n",
      "         1.7499e-06, 8.2494e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         2.1054e-01, 4.4867e-06, 4.1494e-07, 3.3807e-06, 1.1637e-06, 7.9695e-01,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 2.5036e-01, 3.4725e-06, 3.6271e-01,\n",
      "         3.5948e-05, 2.3311e-06, 6.3273e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.0972e-01, 1.2057e-01, 1.5158e-01, 3.7291e-06, 3.8232e-05,\n",
      "         8.2541e-01, 8.1600e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         8.3590e-01, 5.2916e-01, 1.1192e-06, 1.2545e-05, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 9.9998e-01, 1.0000e+00, 9.9761e-01, 6.1977e-06,\n",
      "         5.5861e-07, 7.0480e-07, 1.8539e-06, 9.3604e-07, 1.4782e-01, 1.0000e+00,\n",
      "         6.2705e-01, 1.0310e-01, 2.8808e-07, 2.4258e-06, 4.6458e-06, 1.7657e-09,\n",
      "         1.1300e-05, 5.6264e-06, 1.8547e-06, 5.4111e-01, 7.7886e-01, 1.4986e-01,\n",
      "         4.9215e-07, 1.1850e-05, 2.1590e-05, 6.4651e-08, 2.0027e-05, 6.8330e-07,\n",
      "         1.7046e-07, 1.9766e-06, 5.4970e-07, 1.7134e-07, 7.3006e-07, 2.0443e-06,\n",
      "         3.0805e-05, 4.7731e-06, 8.4745e-08, 3.8882e-01, 9.3712e-02, 5.4297e-08,\n",
      "         9.2587e-06, 1.8022e-06, 8.9407e-06, 1.4082e-06, 1.3083e-04, 3.0657e-06,\n",
      "         1.8806e-05, 3.5590e-01, 8.1632e-01, 1.6467e-01, 9.9047e-01, 8.2053e-01,\n",
      "         3.0042e-06, 4.3506e-06, 9.3229e-08, 2.2467e-05, 9.4015e-08, 1.0000e+00,\n",
      "         1.0000e+00, 7.6172e-01, 1.0000e+00, 3.6922e-01, 8.2559e-07, 9.5167e-06,\n",
      "         9.9932e-06, 9.0797e-05, 1.8848e-06, 9.9967e-01, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.3550e-01, 2.5744e-05, 1.2166e-01, 8.7400e-06, 3.1891e-07,\n",
      "         2.3898e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.5414e-06,\n",
      "         2.4431e-06, 4.5316e-01, 7.5123e-08, 1.4384e-04, 3.4037e-01, 8.2081e-01,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.1269e-01, 1.1692e-06,\n",
      "         8.4035e-06, 1.3653e-05, 4.4174e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         7.9566e-01, 8.2447e-01, 1.5121e-01, 1.9974e-06, 8.2414e-07, 1.0764e-06,\n",
      "         5.0378e-05, 3.1859e-08, 1.6012e-06, 1.0000e+00, 4.3650e-01, 2.7013e-01,\n",
      "         1.4280e-06, 3.3103e-08, 7.1273e-06, 6.7131e-06, 9.6503e-05, 3.4730e-05,\n",
      "         2.3637e-06, 1.3893e-05, 2.4945e-05, 6.5086e-07, 5.4513e-06, 5.3448e-06,\n",
      "         1.9439e-09, 8.0065e-06]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x = x.view(batch_size, x_dim)\n",
    "x = x.to(DEVICE)\n",
    "print('Input: ', x, '\\n')\n",
    "x_hat, mean, log_var = autoencoder(x)\n",
    "print('Modelled hidden: \\n mean: ', mean, '\\n log variance: ', log_var, '\\n decoded to original:', x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6911b256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f7bd8",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45145767",
   "metadata": {},
   "source": [
    "#### Amend the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd320768",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_list = [x.view(1, x_dim).to(DEVICE) for x in demand_list_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a60fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_representation = [torch.cat(autoencoder.Encoder(x.view(1, x_dim).to(DEVICE))).detach() for x in demand_list_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d101565a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_representation[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fae2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = utils.WholeDataset(new_representation, results_list_tensors)\n",
    "train_size = int(0.8 * len(whole_dataset))\n",
    "test_size = len(whole_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(new_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c881d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_dataset_encoded.pt')\n",
    "torch.save(test_dataset, 'test_dataset_encoded.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551dd8e9",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f698d",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1a0050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, padding=0):\n",
    "        super().__init__()\n",
    "        self.padding = padding\n",
    "        self.conv1 = nn.Conv1d(2, 8, 4, padding=self.padding, stride=2)\n",
    "        self.conv2 = nn.Conv1d(8, 32, 3, padding=self.padding, stride=1)\n",
    "        self.conv1_drop = nn.Dropout2d()\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(224, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_drop(self.conv1(x)))\n",
    "        x = F.relu(self.conv2_drop(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4fa7c6",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f533660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 30\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb81ac",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ed41b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a68ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ####### Epoch: 9 #######\n",
      "Train loss: 0.0055\n",
      "Test loss: 0.0318\n",
      "\n",
      "\n",
      " ####### Epoch: 19 #######\n",
      "Train loss: 0.0058\n",
      "Test loss: 0.0122\n",
      "\n",
      "\n",
      " ####### Epoch: 29 #######\n",
      "Train loss: 0.0054\n",
      "Test loss: 0.0100\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.train_net(net, criterion, optimizer, train_loader, test_loader, epochs, use_gpu=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
